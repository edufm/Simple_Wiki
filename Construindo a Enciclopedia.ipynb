{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo uma enciclopedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarefas:\n",
    "\n",
    "1 . Leia os documentos do corpus `reuters` e monte uma lista de palavras que:\n",
    "- Passe todas as palavras para letra minuscula\n",
    "- tem mais que 5 contagens,\n",
    "- tem mais que 2 caracteres, e\n",
    "- não contem digitos\n",
    "\n",
    "2 . Ordene a lista de palavras e grave no disco. Abra em um editor de texto e explore a lista de palavras. Implemente medidas extras de limpeza do vocabulário.\n",
    "\n",
    "3 . Agora sua tarefa é limpar cada texto associado às várias palavras do nosso vocabulário usando seu arsenal de ferramentas de NLP!\n",
    "Para cada texto:\n",
    "- Limpe o texto para remover caracteres estranhos.\n",
    "- Divida o texto em sentenças usando o tokenizador Punkt https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "- Pense em uma forma de identificar a primeira sentença do texto que não seja \"estranha\" (como os cabeçalhos de documento)\n",
    "- Use um modelo de linguagem treinado no corpus reuters.\n",
    "- Pense em usar a perplexidade de cada sentença como medida de \"estranheza\" da sentença.\n",
    "- Talvez usar a primeira sentença de perplexidade \"baixa\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializa o código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz os imports relevantes\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE, Laplace\n",
    "\n",
    "import requests\n",
    "\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import math\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa o dataset de palavras a serem estudadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria a lista de palavras\n",
    "words = list(map(str.lower, reuters.words()))\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atico',\n",
       " 'tax',\n",
       " 'utilities',\n",
       " 'tactics',\n",
       " 'retreat',\n",
       " 'nat',\n",
       " 'centers',\n",
       " 'imbalances',\n",
       " 'merely',\n",
       " 'reaffirmation']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtra as palavras de acordo com os critérios definidos\n",
    "freq_reuters = nltk.FreqDist(words)\n",
    "filtered = list(set(word for word in words if (freq_reuters[word] > 5 and len(word) > 2 and re.fullmatch(r\"[a-z]*\", word))))\n",
    "\n",
    "filtered[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordena a lista e salva em um arquivo para evitar uma nova leitura\n",
    "filtered = sorted(filtered)\n",
    "\n",
    "with open(\"storage/good_words.txt\", \"w\") as dump:\n",
    "    for word in filtered:\n",
    "        dump.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faz os downloads das paginas da wikipédia para construir o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le o bloco de notas com as palavras\n",
    "with open(\"storage/good_words.txt\", \"r\") as load:\n",
    "    good_words = load.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os parametros para a API\n",
    "BATCH_SIZE = 50\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"revisions\",\n",
    "    \"rvprop\": \"content\",\n",
    "    \"rvslots\": \"main\",\n",
    "    \"rvsection\": \"0\",\n",
    "    \"titles\": \"\",\n",
    "    \"format\": \"json\",\n",
    "}\n",
    "\n",
    "# Define uma função para dividir os batches\n",
    "def split_batches(words, batch_size=BATCH_SIZE):\n",
    "    k = 0\n",
    "    while k < len(words):\n",
    "        yield words[k:(k + batch_size)]\n",
    "        k += batch_size\n",
    "\n",
    "main_texts = {}\n",
    "error_log = []\n",
    "\n",
    "# Inicia a seção\n",
    "S = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9179 palavras boas para baixar\n",
      "\n",
      " Processando batch #00184\n",
      "Download concluido\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(good_words)} palavras boas para baixar\")\n",
    "print()\n",
    "\n",
    "# realiza o fetch para cada batch das palavras inicialmente boas\n",
    "for k, batch in enumerate(split_batches(good_words)):\n",
    "    try:\n",
    "        print(f'\\rProcessando batch #{k + 1:05}', end='')\n",
    "        PARAMS['titles'] = '|'.join(batch)\n",
    "        r = S.get(url=URL, params=PARAMS)\n",
    "        r_json = r.json()\n",
    "\n",
    "        # Reverse map of normalized titles.\n",
    "        title_map = {}\n",
    "        for item in r_json['query']['normalized']:\n",
    "            title_map[item['to']] = item['from']\n",
    "            \n",
    "        # Get texts.\n",
    "        texts = {}\n",
    "        for pageid, page_content in r_json['query']['pages'].items():\n",
    "            if int(pageid) < 0:\n",
    "                continue\n",
    "            text = page_content['revisions'][0]['slots']['main']['*']\n",
    "            if page_content['title'] in title_map:\n",
    "                w = title_map[page_content['title']]\n",
    "            else:\n",
    "                w = page_content['title']\n",
    "                \n",
    "            texts[w] = text\n",
    "\n",
    "        # Add to global dict.\n",
    "        main_texts.update(texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log.append((e, r))\n",
    "        \n",
    "print()\n",
    "print()\n",
    "print(\"Download concluido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fução para limpar as palavras encontradas no redirected\n",
    "def treat_redirected(text):\n",
    "    \n",
    "    text = text.split(\"[\")[-1].split(\"]\")[0].split(\" (\")[0]\n",
    "    text = text.strip(\" \")\n",
    "    \n",
    "    if (re.fullmatch(r\"[a-z_ ]*\", text.lower())):\n",
    "        return text\n",
    "    \n",
    "    return \"Remove Me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2169 palavras redirecionadas para baixar\n",
      "\n",
      " Processando batch #00044\n",
      "Download concluido\n"
     ]
    }
   ],
   "source": [
    "# Procura nos textos recebidos palavras que não tiveram a pagina enontrada\n",
    "redirected_words = list(set(treat_redirected(main_texts[word]) for word in main_texts.keys() if \"#redirect\" in main_texts[word][:50].lower()))\n",
    "try:\n",
    "    redirect.remove(\"Remove Me\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"{len(redirected_words)} palavras redirecionadas para baixar\")\n",
    "print()\n",
    "\n",
    "# realiza o fetch para cada batch das palavras redirecionadas\n",
    "for k, batch in enumerate(split_batches(redirected_words)):\n",
    "    try:\n",
    "        print(f'\\rProcessando batch #{k + 1:05}', end='')\n",
    "        PARAMS['titles'] = '|'.join(batch)\n",
    "        r = S.get(url=URL, params=PARAMS)\n",
    "        r_json = r.json()\n",
    "\n",
    "        # Reverse map of normalized titles.\n",
    "        title_map = {}\n",
    "        for item in r_json['query']['normalized']:\n",
    "            title_map[item['to']] = item['from']\n",
    "            \n",
    "        # Get texts.\n",
    "        texts = {}\n",
    "        for pageid, page_content in r_json['query']['pages'].items():\n",
    "            if int(pageid) < 0:\n",
    "                continue\n",
    "            text = page_content['revisions'][0]['slots']['main']['*']\n",
    "            if page_content['title'] in title_map:\n",
    "                w = title_map[page_content['title']]\n",
    "            else:\n",
    "                w = page_content['title']\n",
    "                \n",
    "            texts[w.lower] = text\n",
    "\n",
    "        # Add to global dict.\n",
    "        main_texts.update(texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log.append((e, r))\n",
    "        \n",
    "print()\n",
    "print()\n",
    "print(\"Download concluido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2882 paginas redirecionadas para remover\n",
      "3049 paginas de desambiguação para remover\n",
      "31 paginas com erro\n"
     ]
    }
   ],
   "source": [
    "# Retira da lista palavras que foram redirecionadas\n",
    "words = main_texts.keys()\n",
    "not_good_data = list(set(word for word in words if \"#redirect\" in main_texts[word][:50].lower()))\n",
    "print(f\"{len(not_good_data)} paginas redirecionadas para remover\")\n",
    "for word in not_good_data:\n",
    "    main_texts.pop(word)\n",
    "    \n",
    "# Retira palavras que vão para as paginas de desambiguação\n",
    "words = main_texts.keys()\n",
    "not_good_data = list(set(word for word in words if \"may refer to:\" in main_texts[word]))\n",
    "print(f\"{len(not_good_data)} paginas de desambiguação para remover\")\n",
    "for word in not_good_data:\n",
    "    main_texts.pop(word)\n",
    "    \n",
    "# Retira palavras que vão para as paginas muito curtas\n",
    "words = main_texts.keys()\n",
    "not_good_data = list(set(word for word in words if \"{{Short pages monitor}}\" in main_texts[word]))\n",
    "not_good_data = list(set(not_good_data + [word for word in words if \"{{wiktionary redirect}}\" in main_texts[word]]))\n",
    "\n",
    "print(f\"{len(not_good_data)} paginas com erro\")\n",
    "for word in not_good_data:\n",
    "    main_texts.pop(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exporta os resultados para arquivos\n",
    "with open('storage/texts.json', 'w') as f:\n",
    "    json.dump(main_texts, f, indent=4)\n",
    "\n",
    "with open('storage/errors.txt', 'w') as f:\n",
    "    for e, r in error_log:\n",
    "        f.write(f'{e} ({type(e)})\\nConteudo:\\n{r.headers}\\n{\"*\"*100}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realia a limpeza dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le o bloco de notas com as palavras\n",
    "with open(\"storage/texts.json\", \"r\") as load:\n",
    "    main_texts = json.load(load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstração da limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"hanover\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a variavel de teste\n",
    "clean = main_texts[sample]\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove referencias do HTML\n",
    "clean = re.sub(r\"<ref.*?(/ref>|/>)\", \"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean = re.sub(r\"<sup.*?(/sup>|/>)\", \"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove marcas de objetos desenhados na pagina (entre chaves)\n",
    "clean = re.sub(r\"\\{\\{(?:[^\\'\\'\\'])*?\\}\\}\", \"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corta as legendas pré texto\n",
    "splited_clean = clean.split(\"'''\")[1:]\n",
    "\n",
    "if len(splited_clean) == 0:\n",
    "    pass\n",
    "else:\n",
    "    clean = \"\".join(splited_clean)\n",
    "\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subistitui palavras entre colxete pela própria palavra\n",
    "clean = re.sub(r\"\\[\\[((?:[^|])*?)\\]\\]\", r\"\\1\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subistitui palavras entre colxete com pipe pela palavra depois do pipe\n",
    "clean = re.sub(r\"\\[\\[(?:[^|]|)*(.*?)\\]\\]\", r\"\\1\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limpa algumas outras variaveis irrelevantes\n",
    "clean = re.sub(r\"\\n|\\'\", r\"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean = re.sub(r\"}}\", r\"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean = re.sub(r\"\\|\", r\"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean = re.sub(r\"\\(;|\\(,\", r\"(SPECIALCHAR\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean = re.sub(r\"\\(SPECIALCHAR.*?\\)\", \"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(clean, remove_special=True):\n",
    "    # Remove referencias do HTML\n",
    "    clean = re.sub(r\"<ref.*?(/ref>|/>)\", \"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "    clean = re.sub(r\"<sup.*?(/sup>|/>)\", \"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "\n",
    "    # Remove marcas de objetos desenhados na pagina (entre chaves)\n",
    "    clean = re.sub(r\"\\{\\{(?:[^\\'\\'\\'])*?\\}\\}\", \"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "\n",
    "    # Corta as legendas pré texto\n",
    "    splited_clean = clean.split(\"'''\")[1:]\n",
    "\n",
    "    if len(splited_clean) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        clean = \"\".join(splited_clean)\n",
    "\n",
    "    # Subistitui palavras entre colxete pela própria palavra\n",
    "    clean = re.sub(r\"\\[\\[((?:[^|])*?)\\]\\]\", r\"\\1\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "\n",
    "    # Subistitui palavras entre colxete com pipe pela palavra depois do pipe\n",
    "    clean = re.sub(r\"\\[\\[(?:[^|]|)*(.*?)\\]\\]\", r\"\\1\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "\n",
    "    # limpa algumas outras variaveis irrelevantes\n",
    "    clean = re.sub(r\"\\n|\\'\", r\"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "    clean = re.sub(r\"}}\", r\"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "    clean = re.sub(r\"\\|\", r\"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "    clean = re.sub(r\"\\(;|\\(,\", r\"(SPECIALCHAR\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "    \n",
    "    if remove_special:\n",
    "        clean = re.sub(r\"\\(SPECIALCHAR.*?\\)\", \"\", clean, flags=re.DOTALL|re.MULTILINE)\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(main_texts.keys()):\n",
    "    \n",
    "    new_text = clean_string(main_texts[word])\n",
    "    \n",
    "    if len(new_text) == 0:\n",
    "        main_texts.pop(word)\n",
    "    else:\n",
    "        main_texts[word] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta os resultados para arquivos\n",
    "with open('storage/clean_texts.json', 'w') as f:\n",
    "    json.dump(main_texts, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide o texto em sentenças usando o tokenizador Punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le o bloco de notas com as palavras\n",
    "with open(\"storage/clean_texts.json\", \"r\") as load:\n",
    "    main_texts = json.load(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"hanover\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_sents = {}\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "for key, value in main_texts.items():\n",
    "    main_sents[key] = sent_tokenizer.tokenize(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hanover or Hannover  is the capital and largest city of the German state of Lower Saxony.',\n",
       " 'Its 535,061 (2017) inhabitants make it the thirteenth-largest city in Germany as well as the third-largest city in Northern Germany after Hamburg and Bremen.',\n",
       " 'The city lies at the confluence of the River Leine (progression: ) and its tributary Ihme, in the south of the North German Plain, and is the largest city in the Hannover–Braunschweig–Göttingen–Wolfsburg Metropolitan Region.',\n",
       " 'It is the fifth-largest city in the Low German dialect area after Hamburg, Dortmund, Essen and Bremen.Before it became the capital of Lower Saxony in 1946 Hanover was the capital of the Principality of Calenberg (1636–1692), the Electorate of Hanover (1692–1814), the Kingdom of Hanover (1814–1866), the Province of Hanover of the Kingdom of Prussia (1868–1918), the Province of Hanover of the Free State of Prussia (1918–1946) and of the State of Hanover (1946).',\n",
       " 'From 1714 to 1837 Hanover was by personal union the family seat of the Hanoverian Kings of the United Kingdom of Great Britain and Ireland, under their title of the dukes of Brunswick-Lüneburg (later described as the Elector of Hanover).The city is a major crossing point of railway lines and motorways (Autobahnen), connecting European main lines in both the east-west (Berlin–Ruhr area/Düsseldorf/Cologne) and north-south (Hamburg–Frankfurt/Stuttgart/Munich) directions.',\n",
       " 'Hannover Airport lies north of the city, in Langenhagen, and is Germanys ninth-busiest airport.',\n",
       " 'The citys most notable institutes of higher education are the Hannover Medical School with its university hospital (Klinikum der Medizinischen Hochschule Hannover) and the Leibniz University Hannover.The Hanover fairground, owing to numerous extensions, especially for the Expo 2000, is the largest in the world.',\n",
       " 'Hanover hosts annual commercial trade fairs such as the Hanover Fair and up to 2018 the CeBIT.',\n",
       " 'The IAA Commercial Vehicles show takes place every two years.',\n",
       " 'It is the worlds leading trade show for transport, logistics and mobility.',\n",
       " 'Every year Hanover hosts the Schützenfest Hannover, the worlds largest marksmens festival, and the Oktoberfest Hannover.Hanover is the traditional English spelling.',\n",
       " 'The German spelling (with a double n) is becoming more popular in English; recent editions of encyclopedias prefer the German spelling, and the local government uses the German spelling on English websites.',\n",
       " 'The English pronunciation, with stress on the first syllable, is applied to both the German and English spellings, which is different from German pronunciation, with stress on the second syllable and a long second vowel.',\n",
       " 'The traditional English spelling is still used in historical contexts, especially when referring to the British House of Hanover.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_sents[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta os resultados para arquivos\n",
    "with open('storage/def_sents.json', 'w') as f:\n",
    "    json.dump(main_sents, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise por Perplexidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le o bloco de notas com as palavras\n",
    "with open(\"storage/def_sents.json\", \"r\") as load:\n",
    "    main_sents = json.load(load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando o corpus reuters de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepraração dos dados\n",
    "reuters_sentences = reuters.sents()\n",
    "reuters_train, reuters_vocab = padded_everygram_pipeline(2, reuters_sentences)\n",
    "\n",
    "reuters_train = list(list(t) for t in reuters_train)\n",
    "reuters_vocab = list(reuters_vocab)\n",
    "\n",
    "#Treinamento do Modelo\n",
    "lm_reuters = Laplace(2)\n",
    "lm_reuters.fit(reuters_train, reuters_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"hanover\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3430.347882606758\n",
      "8372.738449782717\n",
      "3529.6318222835257\n",
      "7686.65569545711\n",
      "10478.996008500379\n",
      "14306.610566047464\n",
      "13130.070098942655\n",
      "7638.787064473866\n",
      "11221.220325754444\n",
      "5488.853729160141\n",
      "19418.311305672054\n",
      "10551.84100606817\n",
      "4051.3891598621776\n",
      "5014.145133997854\n"
     ]
    }
   ],
   "source": [
    "# Encontra as perplexidades da amostra\n",
    "text_sentences = [t.split() for t in main_sents[sample]]\n",
    "test, _ = padded_everygram_pipeline(2, text_sentences)\n",
    "test = list(list(t) for t in test)\n",
    "\n",
    "# Calcula a perplexidade das sentenças\n",
    "idx = 0 \n",
    "min_value = math.inf\n",
    "for i, s in enumerate(test):\n",
    "    px = lm_reuters.perplexity(s)\n",
    "    \n",
    "    if(min_value > px):\n",
    "        min_value = px\n",
    "        idx = i\n",
    "\n",
    "    print(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lowest_perplexity_reuter_parallel(main_sents, words, do_print=False):\n",
    "    \n",
    "    encyclopedia_reuters = {}\n",
    "    for i, word in enumerate(words):\n",
    "        idx, min_value = find_lowest_perplexity_reuters(main_sents, word)\n",
    "        encyclopedia_reuters[word] = main_sents[word][idx]\n",
    "        \n",
    "    return encyclopedia_reuters\n",
    "        \n",
    "def find_lowest_perplexity_reuters(main_sents, word, do_print=False):\n",
    "\n",
    "    # Prepraração dos dados\n",
    "    text_sentences = [t.split() for t in main_sents[word]]\n",
    "    test, _ = padded_everygram_pipeline(2, text_sentences)\n",
    "    test = list(list(t) for t in test)\n",
    "\n",
    "    # Calcula a perplexidade das sentenças\n",
    "    idx = 0 \n",
    "    min_value = math.inf\n",
    "    for i, s in enumerate(test):\n",
    "        px = lm_reuters.perplexity(s)\n",
    "\n",
    "        if(min_value > px):\n",
    "            min_value = px\n",
    "            idx = i\n",
    "\n",
    "        if do_print:\n",
    "            print(px)\n",
    "            \n",
    "    return idx, min_value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando em serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 out of 3832"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-f806098552ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_lowest_perplexity_reuters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_sents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mencyclopedia_reuters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_sents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-171-401046f3ada2>\u001b[0m in \u001b[0;36mfind_lowest_perplexity_reuters\u001b[1;34m(main_sents, word, do_print)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmin_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlm_reuters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_value\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mperplexity\u001b[1;34m(self, text_ngrams)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \"\"\"\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ngrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mentropy\u001b[1;34m(self, text_ngrams)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \"\"\"\n\u001b[0;32m    189\u001b[0m         return -1 * _mean(\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \"\"\"\n\u001b[0;32m    189\u001b[0m         return -1 * _mean(\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mlogscore\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlog_base2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcontext_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \"\"\"\n\u001b[0;32m    142\u001b[0m         return self.unmasked_score(\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         )\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\models.py\u001b[0m in \u001b[0;36munmasked_score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mword_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mnorm_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword_count\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnorm_count\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\vocabulary.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;34m\"\"\"Computing size of vocabulary reflects the cutoff.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\vocabulary.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;34m\"\"\"Computing size of vocabulary reflects the cutoff.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\vocabulary.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    222\u001b[0m         vocabulary.\"\"\"\n\u001b[0;32m    223\u001b[0m         return chain(\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munk_label\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         )\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\vocabulary.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \"\"\"Only consider items with counts GE to cutoff as being in the\n\u001b[0;32m    217\u001b[0m         vocabulary.\"\"\"\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\nltk\\lm\\vocabulary.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cutoff\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munk_label\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encyclopedia_reuters = {}\n",
    "words = list(main_sents.keys())\n",
    "total = len(words)\n",
    "for i, word in enumerate(words):\n",
    "    idx, min_value = find_lowest_perplexity_reuters(main_sents, word)\n",
    "    encyclopedia_reuters[word] = main_sents[word][idx]\n",
    "    \n",
    "    print(f\"\\r{i+1} out of {total}\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando em Paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 12\n",
    "encyclopedia_reuters = {}\n",
    "words = list(main_sents.keys())\n",
    "part = round(len(words)/n_jobs)\n",
    "\n",
    "words_batch = []\n",
    "for i in range(n_jobs):\n",
    "    if ((i+1)*part > len(words)):\n",
    "        words_batch.append(words[i*part:])\n",
    "    else:\n",
    "        words_batch.append(words[i*part:(i+1)*part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encyclopedia_reuters = {}\n",
    "results = Parallel(n_jobs=n_jobs, verbose=50)(delayed(find_lowest_perplexity_reuter_parallel)(main_sents, words) for words in words_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta os resultados para arquivos\n",
    "with open('storage/encyclopedia_reuters.json', 'w') as f:\n",
    "    json.dump(encyclopedia_reuters, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o próprio texto de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"hanover\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.7550009448657\n",
      "124.01750308067002\n",
      "93.11147167318235\n",
      "83.93973690163187\n",
      "121.94755697767927\n",
      "110.54408863750604\n",
      "128.84281222783548\n",
      "116.52755746298136\n",
      "150.0887349015754\n",
      "113.86013794070298\n",
      "106.94385975498913\n",
      "117.51953388139117\n",
      "114.44498585763819\n",
      "123.91459807786784\n"
     ]
    }
   ],
   "source": [
    "# Prepraração dos dados\n",
    "text_sentences = [t.split() for t in main_sents[sample]]\n",
    "train, vocab = padded_everygram_pipeline(2, text_sentences)\n",
    "\n",
    "train = list(list(t) for t in train)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Treinamento do Modelo\n",
    "lm = Laplace(2)\n",
    "lm.fit(train, vocab)\n",
    "\n",
    "# Calcula a perplexidade das sentenças\n",
    "idx = 0 \n",
    "min_value = math.inf\n",
    "for i, s in enumerate(train):\n",
    "    px = lm.perplexity(s)\n",
    "    \n",
    "    if(min_value > px):\n",
    "        min_value = px\n",
    "        idx = i\n",
    "\n",
    "    print(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lowest_perplexity_same_parallel(main_sents, words, do_print=False):\n",
    "    \n",
    "    encyclopedia_same = {}\n",
    "    for i, word in enumerate(words):\n",
    "        idx, min_value = find_lowest_perplexity_same(main_sents, word)\n",
    "        encyclopedia_same[word] = main_sents[word][idx]\n",
    "        \n",
    "    return encyclopedia_same\n",
    "\n",
    "def find_lowest_perplexity_same(main_sents, word, do_print=False):\n",
    "\n",
    "    # Prepraração dos dados\n",
    "    text_sentences = [t.split() for t in main_sents[word]]\n",
    "    train, vocab = padded_everygram_pipeline(2, text_sentences)\n",
    "\n",
    "    train = list(list(t) for t in train)\n",
    "    vocab = list(vocab)\n",
    "\n",
    "    # Treinamento do Modelo\n",
    "    lm = Laplace(2)\n",
    "    lm.fit(train, vocab)\n",
    "    \n",
    "    # Calcula a perplexidade das sentenças\n",
    "    idx = 0 \n",
    "    min_value = math.inf\n",
    "    for i, s in enumerate(train):\n",
    "        px = lm.perplexity(s)\n",
    "        \n",
    "        if(min_value > px):\n",
    "            min_value = px\n",
    "            idx = i\n",
    "        \n",
    "        if do_print:\n",
    "            print(px)\n",
    "            \n",
    "    return idx, min_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3832 out of 3832"
     ]
    }
   ],
   "source": [
    "encyclopedia_same = {}\n",
    "words = list(main_sents.keys())\n",
    "total = len(words)\n",
    "for i, word in enumerate(words):\n",
    "    idx, min_value = find_lowest_perplexity_same(main_sents, word)\n",
    "    encyclopedia_same[word] = main_sents[word][idx]\n",
    "    \n",
    "    print(f\"\\r{i+1} out of {total}\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando em paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 12\n",
    "encyclopedia_reuters = {}\n",
    "words = list(main_sents.keys())\n",
    "part = round(len(words)/n_jobs)\n",
    "\n",
    "words_batch = []\n",
    "for i in range(n_jobs):\n",
    "    if ((i+1)*part > len(words)):\n",
    "        words_batch.append(words[i*part:])\n",
    "    else:\n",
    "        words_batch.append(words[i*part:(i+1)*part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=12)]: Done   2 out of  12 | elapsed:  1.1min remaining:  5.6min\n",
      "[Parallel(n_jobs=12)]: Done   3 out of  12 | elapsed:  1.1min remaining:  3.4min\n",
      "[Parallel(n_jobs=12)]: Done   4 out of  12 | elapsed:  1.1min remaining:  2.3min\n",
      "[Parallel(n_jobs=12)]: Done   5 out of  12 | elapsed:  1.2min remaining:  1.7min\n",
      "[Parallel(n_jobs=12)]: Done   6 out of  12 | elapsed:  1.2min remaining:  1.2min\n",
      "[Parallel(n_jobs=12)]: Done   7 out of  12 | elapsed:  1.2min remaining:   53.0s\n",
      "[Parallel(n_jobs=12)]: Done   8 out of  12 | elapsed:  1.3min remaining:   37.5s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of  12 | elapsed:  1.3min remaining:   25.3s\n",
      "[Parallel(n_jobs=12)]: Done  10 out of  12 | elapsed:  1.3min remaining:   15.2s\n",
      "[Parallel(n_jobs=12)]: Done  12 out of  12 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  12 out of  12 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "encyclopedia_same = {}\n",
    "results = Parallel(n_jobs=n_jobs, verbose=50)(delayed(find_lowest_perplexity_same_parallel)(main_sents, words) for words in words_batch)\n",
    "\n",
    "for result in results:\n",
    "    encyclopedia_same.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta os resultados para arquivos\n",
    "with open('storage/encyclopedia_same.json', 'w') as f:\n",
    "    json.dump(encyclopedia_same, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explora as encyclopedias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'storage/encyclopedia_reuters.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-a5d0aa933f14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mencyclopedia_same\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage/encyclopedia_reuters.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mencyclopedia_reuters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'storage/encyclopedia_reuters.json'"
     ]
    }
   ],
   "source": [
    "# Le o bloco de notas com os dados\n",
    "with open(\"storage/encyclopedia_same.json\", \"r\") as load:\n",
    "    encyclopedia_same = json.load(load)\n",
    "    \n",
    "with open(\"storage/encyclopedia_reuters.json\", \"r\") as load:\n",
    "    encyclopedia_reuters = json.load(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hanover or Hannover  is the capital and largest city of the German state of Lower Saxony.'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encyclopedia_same[\"hanover\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encyclopedia_reuters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-50394034c497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencyclopedia_reuters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hanover\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encyclopedia_reuters' is not defined"
     ]
    }
   ],
   "source": [
    "encyclopedia_reuters[\"hanover\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
